version: '3.8'

services:
  # Redis Cache Service
  redis:
    image: redis:7-alpine
    container_name: llm-bridge-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - llm-bridge-net

  # LLM Bridge API Service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-bridge-api
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - REDIS_URL=redis://redis:6379/0
      - CACHE_TTL_SECONDS=${CACHE_TTL_SECONDS:-86400}
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./logs:/app/logs
      - ./workflows:/app/workflows:ro
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - llm-bridge-net
    restart: unless-stopped

  # Optional: Streamlit Dashboard
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
    container_name: llm-bridge-dashboard
    ports:
      - "8501:8501"
    env_file:
      - .env
    environment:
      - API_BASE_URL=http://api:8000
    depends_on:
      api:
        condition: service_healthy
    networks:
      - llm-bridge-net
    profiles:
      - full
    restart: unless-stopped

networks:
  llm-bridge-net:
    driver: bridge

volumes:
  redis-data: